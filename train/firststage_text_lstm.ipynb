{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using lstm to text data classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import xgboost as xgb\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import keras.preprocessing.text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import keras\n",
    "# from keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, SpatialDropout1D, Embedding, LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7200 unique tokens.\n",
      "Shape of data tensor: (43255, 20)\n",
      "Shape of label tensor: (43255, 27)\n"
     ]
    }
   ],
   "source": [
    "# train = pd.read_csv(\"/kaggle/input/uw-cs480-fall20/train.csv\")\n",
    "# test = pd.read_csv(\"/kaggle/input/uw-cs480-fall20/test.csv\")\n",
    "\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "data = pd.DataFrame(pd.concat([train, test]))\n",
    "\n",
    "# text processing\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "\n",
    "# put gender/color etc info into 'text'\n",
    "data[\"text\"] = data['gender'] + \" \" + data[\"baseColour\"]+ \" \" + data[\"season\"]+ \" \" + data[\"usage\"] + \" \"+ data['noisyTextDescription']\n",
    "\n",
    "data['TextDescription'] = data['text'].apply(clean_text)\n",
    "data['TextDescription'] = data['TextDescription'].str.replace('\\d+', '')\n",
    "# data.head()\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(data['TextDescription'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X = tokenizer.texts_to_sequences(data['TextDescription'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "\n",
    "\n",
    "Y = pd.get_dummies(data['category']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Accessories', 'Apparel Set', 'Bags', 'Belts', 'Bottomwear',\n",
       "       'Cufflinks', 'Dress', 'Eyewear', 'Flip Flops', 'Fragrance',\n",
       "       'Free Gifts', 'Headwear', 'Innerwear', 'Jewellery', 'Lips',\n",
       "       'Loungewear and Nightwear', 'Makeup', 'Nails', 'Sandal', 'Saree',\n",
       "       'Scarves', 'Shoes', 'Socks', 'Ties', 'Topwear', 'Wallets', 'Watches'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(data['category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # loading\n",
    "# with open('tokenizer.pickle', 'rb') as handle:\n",
    "#     tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17300, 27) (4327, 27)\n"
     ]
    }
   ],
   "source": [
    "X_train_total, X_test = pd.DataFrame(X[:len(train)]), pd.DataFrame(X[len(train):])\n",
    "Y_train_total, Y_test = pd.DataFrame(Y[:len(train)]), pd.DataFrame(Y[len(train):])\n",
    "\n",
    "val_size = int(len(train)/5)\n",
    "\n",
    "X_train1 = X_train_total[val_size:]\n",
    "X_val1 = X_train_total[:val_size]\n",
    "\n",
    "X_train2 = pd.concat([X_train_total.iloc[:val_size,], X_train_total.iloc[val_size*2:]])\n",
    "X_val2 = X_train_total[val_size:val_size*2]\n",
    "\n",
    "X_train3 = pd.concat([X_train_total.iloc[:val_size*2,], X_train_total.iloc[val_size*3:]])\n",
    "X_val3 = X_train_total[val_size*2:val_size*3]\n",
    "\n",
    "X_train4 = pd.concat([X_train_total.iloc[:val_size*3,], X_train_total.iloc[val_size*4:]])\n",
    "X_val4 = X_train_total[val_size*3:val_size*4]\n",
    "\n",
    "X_train5 = X_train_total.iloc[:val_size*4,]\n",
    "X_val5 = X_train_total[val_size*4:]\n",
    "\n",
    "Y_train1 = Y_train_total[val_size:]\n",
    "Y_val1 = Y_train_total[:val_size]\n",
    "\n",
    "Y_train2 = pd.concat([Y_train_total.iloc[:val_size,], Y_train_total.iloc[val_size*2:]])\n",
    "Y_val2 = Y_train_total[val_size:val_size*2]\n",
    "\n",
    "Y_train3 = pd.concat([Y_train_total.iloc[:val_size*2,], Y_train_total.iloc[val_size*3:]])\n",
    "Y_val3 = Y_train_total[val_size*2:val_size*3]\n",
    "\n",
    "Y_train4 = pd.concat([Y_train_total.iloc[:val_size*3,], Y_train_total.iloc[val_size*4:]])\n",
    "Y_val4 = Y_train_total[val_size*3:val_size*4]\n",
    "\n",
    "Y_train5 = Y_train_total.iloc[:val_size*4,]\n",
    "Y_val5 = Y_train_total[val_size*4:]\n",
    "\n",
    "print(Y_train5.shape,Y_val5.shape)\n",
    "# print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, Y_train, X_val, Y_val, k):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(27, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    epochs = 3 \n",
    "    batch_size = 64\n",
    "\n",
    "    # create a callback that will save the best model while training\n",
    "    save_best_model = ModelCheckpoint(\"best_model_\" + str(k) + '.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "\n",
    "    history = model.fit(X_train, Y_train, batch_size=batch_size, \\\n",
    "                        epochs=epochs,validation_data=(X_val, Y_val),shuffle=True,callbacks=[save_best_model])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "271/271 [==============================] - ETA: 0s - loss: 1.2107 - accuracy: 0.6797\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.87630, saving model to best_model_1.h5\n",
      "271/271 [==============================] - 54s 200ms/step - loss: 1.2107 - accuracy: 0.6797 - val_loss: 0.4675 - val_accuracy: 0.8763\n",
      "Epoch 2/10\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.3088 - accuracy: 0.9167\n",
      "Epoch 00002: val_accuracy improved from 0.87630 to 0.89757, saving model to best_model_1.h5\n",
      "271/271 [==============================] - 54s 199ms/step - loss: 0.3088 - accuracy: 0.9167 - val_loss: 0.3758 - val_accuracy: 0.8976\n",
      "Epoch 3/10\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.1465 - accuracy: 0.9629\n",
      "Epoch 00003: val_accuracy improved from 0.89757 to 0.90058, saving model to best_model_1.h5\n",
      "271/271 [==============================] - 54s 200ms/step - loss: 0.1465 - accuracy: 0.9629 - val_loss: 0.3781 - val_accuracy: 0.9006\n",
      "Epoch 4/10\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9825\n",
      "Epoch 00004: val_accuracy did not improve from 0.90058\n",
      "271/271 [==============================] - 60s 222ms/step - loss: 0.0750 - accuracy: 0.9825 - val_loss: 0.4158 - val_accuracy: 0.8994\n",
      "Epoch 5/10\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9894\n",
      "Epoch 00005: val_accuracy did not improve from 0.90058\n",
      "271/271 [==============================] - 62s 228ms/step - loss: 0.0445 - accuracy: 0.9894 - val_loss: 0.4451 - val_accuracy: 0.8978\n",
      "Epoch 6/10\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9939\n",
      "Epoch 00006: val_accuracy did not improve from 0.90058\n",
      "271/271 [==============================] - 55s 204ms/step - loss: 0.0280 - accuracy: 0.9939 - val_loss: 0.4902 - val_accuracy: 0.8916\n",
      "Epoch 7/10\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9955\n",
      "Epoch 00007: val_accuracy did not improve from 0.90058\n",
      "271/271 [==============================] - 54s 200ms/step - loss: 0.0211 - accuracy: 0.9955 - val_loss: 0.5273 - val_accuracy: 0.8936\n",
      "Epoch 8/10\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9966\n",
      "Epoch 00008: val_accuracy did not improve from 0.90058\n",
      "271/271 [==============================] - 55s 204ms/step - loss: 0.0161 - accuracy: 0.9966 - val_loss: 0.5164 - val_accuracy: 0.8906\n",
      "Epoch 9/10\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9980\n",
      "Epoch 00009: val_accuracy did not improve from 0.90058\n",
      "271/271 [==============================] - 56s 206ms/step - loss: 0.0114 - accuracy: 0.9980 - val_loss: 0.5613 - val_accuracy: 0.8923\n",
      "Epoch 10/10\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.9975\n",
      "Epoch 00010: val_accuracy did not improve from 0.90058\n",
      "271/271 [==============================] - 55s 202ms/step - loss: 0.0106 - accuracy: 0.9975 - val_loss: 0.5680 - val_accuracy: 0.8913\n",
      "136/136 [==============================] - 1s 9ms/step - loss: 0.3781 - accuracy: 0.9006\n"
     ]
    }
   ],
   "source": [
    "model_1 = train(X_train1, Y_train1, X_val1, Y_val1, 1)\n",
    "saved_model1 =  load_model('best_model_1.h5')\n",
    "\n",
    "scores_cnn = saved_model1.evaluate(X_val1, Y_val1, verbose=1)\n",
    "X_pred_1 = saved_model1.predict(X_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "271/271 [==============================] - ETA: 0s - loss: 1.2092 - accuracy: 0.6817\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.86844, saving model to best_model_2.h5\n",
      "271/271 [==============================] - 55s 204ms/step - loss: 1.2092 - accuracy: 0.6817 - val_loss: 0.4771 - val_accuracy: 0.8684\n",
      "Epoch 2/3\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.9213\n",
      "Epoch 00002: val_accuracy improved from 0.86844 to 0.90150, saving model to best_model_2.h5\n",
      "271/271 [==============================] - 55s 202ms/step - loss: 0.2996 - accuracy: 0.9213 - val_loss: 0.3645 - val_accuracy: 0.9015\n",
      "Epoch 3/3\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9638\n",
      "Epoch 00003: val_accuracy improved from 0.90150 to 0.90936, saving model to best_model_2.h5\n",
      "271/271 [==============================] - 55s 204ms/step - loss: 0.1446 - accuracy: 0.9638 - val_loss: 0.3638 - val_accuracy: 0.9094\n",
      "136/136 - 1s - loss: 0.3638 - accuracy: 0.9094\n"
     ]
    }
   ],
   "source": [
    "model_2 = train(X_train2, Y_train2, X_val2, Y_val2, 2)\n",
    "saved_model2 =  load_model('best_model_2.h5')\n",
    "scores_cnn = saved_model2.evaluate(X_val2, Y_val2, verbose=2)\n",
    "X_pred_2 = saved_model2.predict(X_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "271/271 [==============================] - ETA: 0s - loss: 1.2652 - accuracy: 0.6687\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.87052, saving model to best_model_3.h5\n",
      "271/271 [==============================] - 60s 222ms/step - loss: 1.2652 - accuracy: 0.6687 - val_loss: 0.4980 - val_accuracy: 0.8705\n",
      "Epoch 2/3\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.9142\n",
      "Epoch 00002: val_accuracy improved from 0.87052 to 0.88994, saving model to best_model_3.h5\n",
      "271/271 [==============================] - 68s 250ms/step - loss: 0.3237 - accuracy: 0.9142 - val_loss: 0.3814 - val_accuracy: 0.8899\n",
      "Epoch 3/3\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.1597 - accuracy: 0.9590\n",
      "Epoch 00003: val_accuracy improved from 0.88994 to 0.90405, saving model to best_model_3.h5\n",
      "271/271 [==============================] - 55s 204ms/step - loss: 0.1597 - accuracy: 0.9590 - val_loss: 0.3670 - val_accuracy: 0.9040\n"
     ]
    }
   ],
   "source": [
    "model_3 = train(X_train3, Y_train3, X_val3, Y_val3, 3)\n",
    "saved_model3 =  load_model('best_model_3.h5')\n",
    "scores_cnn = saved_model3.evaluate(X_val3, Y_val3, verbose=3)\n",
    "X_pred_3 = saved_model3.predict(X_val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "271/271 [==============================] - ETA: 0s - loss: 1.2441 - accuracy: 0.6712\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.87168, saving model to best_model_4.h5\n",
      "271/271 [==============================] - 55s 203ms/step - loss: 1.2441 - accuracy: 0.6712 - val_loss: 0.4528 - val_accuracy: 0.8717\n",
      "Epoch 2/3\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.9177\n",
      "Epoch 00002: val_accuracy improved from 0.87168 to 0.89642, saving model to best_model_4.h5\n",
      "271/271 [==============================] - 56s 205ms/step - loss: 0.3047 - accuracy: 0.9177 - val_loss: 0.3605 - val_accuracy: 0.8964\n",
      "Epoch 3/3\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.9622\n",
      "Epoch 00003: val_accuracy improved from 0.89642 to 0.90382, saving model to best_model_4.h5\n",
      "271/271 [==============================] - 55s 203ms/step - loss: 0.1461 - accuracy: 0.9622 - val_loss: 0.3366 - val_accuracy: 0.9038\n"
     ]
    }
   ],
   "source": [
    "model_4 = train(X_train4, Y_train4, X_val4, Y_val4, 4)\n",
    "saved_model4 =  load_model('best_model_4.h5')\n",
    "scores_cnn = saved_model4.evaluate(X_val4, Y_val4, verbose=4)\n",
    "X_pred_4 = saved_model4.predict(X_val4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "271/271 [==============================] - ETA: 0s - loss: 1.2231 - accuracy: 0.6794\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.87266, saving model to best_model_5.h5\n",
      "271/271 [==============================] - 55s 203ms/step - loss: 1.2231 - accuracy: 0.6794 - val_loss: 0.4867 - val_accuracy: 0.8727\n",
      "Epoch 2/3\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.9192\n",
      "Epoch 00002: val_accuracy improved from 0.87266 to 0.90270, saving model to best_model_5.h5\n",
      "271/271 [==============================] - 55s 202ms/step - loss: 0.3024 - accuracy: 0.9192 - val_loss: 0.3400 - val_accuracy: 0.9027\n",
      "Epoch 3/3\n",
      "271/271 [==============================] - ETA: 0s - loss: 0.1476 - accuracy: 0.9632\n",
      "Epoch 00003: val_accuracy improved from 0.90270 to 0.90294, saving model to best_model_5.h5\n",
      "271/271 [==============================] - 55s 201ms/step - loss: 0.1476 - accuracy: 0.9632 - val_loss: 0.3305 - val_accuracy: 0.9029\n"
     ]
    }
   ],
   "source": [
    "model_5 = train(X_train5, Y_train5, X_val5, Y_val5, 5)\n",
    "saved_model5 =  load_model('best_model_5.h5')\n",
    "scores_cnn = saved_model5.evaluate(X_val5, Y_val5, verbose=5)\n",
    "X_pred_5 = saved_model5.predict(X_val5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "169/169 [==============================] - 34s 188ms/step - loss: 2.2193 - accuracy: 0.4374\n",
      "\n",
      "Epoch 00001: accuracy improved from -inf to 0.58783, saving model to model\\best_model_lstm.h5\n",
      "Epoch 2/5\n",
      "169/169 [==============================] - 30s 180ms/step - loss: 0.4834 - accuracy: 0.8849\n",
      "\n",
      "Epoch 00002: accuracy improved from 0.58783 to 0.89823, saving model to model\\best_model_lstm.h5\n",
      "Epoch 3/5\n",
      "169/169 [==============================] - 30s 180ms/step - loss: 0.2103 - accuracy: 0.9498\n",
      "\n",
      "Epoch 00003: accuracy improved from 0.89823 to 0.95140, saving model to model\\best_model_lstm.h5\n",
      "Epoch 4/5\n",
      "169/169 [==============================] - 29s 171ms/step - loss: 0.1214 - accuracy: 0.9731\n",
      "\n",
      "Epoch 00004: accuracy improved from 0.95140 to 0.97272, saving model to model\\best_model_lstm.h5\n",
      "Epoch 5/5\n",
      "169/169 [==============================] - 30s 179ms/step - loss: 0.0741 - accuracy: 0.9861\n",
      "\n",
      "Epoch 00005: accuracy improved from 0.97272 to 0.98511, saving model to model\\best_model_lstm.h5\n"
     ]
    }
   ],
   "source": [
    "# train with every training data \n",
    "# LSTM for the first stage\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model_lstm.add(SpatialDropout1D(0.2))\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(27, activation='softmax'))\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 5 \n",
    "batch_size = 128\n",
    "\n",
    "save_best_model = ModelCheckpoint(\"model/best_model_lstm.h5\", monitor='accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model_lstm.fit(X_train_total, Y_train_total, batch_size=batch_size, \\\n",
    "                    epochs=epochs,shuffle=True, callbacks=[save_best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first stage predict for test data\n",
    "test_pred = model_lstm.predict(X_test)\n",
    "test_pred_df = pd.DataFrame(test_pred)\n",
    "\n",
    "c = []\n",
    "for i in range(27):\n",
    "    c.append(\"lstm_\"+str(i))\n",
    "    \n",
    "test_pred_df.columns = c\n",
    "test_pred_df.to_csv(\"test_lstm_X.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking\n",
    "\n",
    "train_pred = np.concatenate((X_pred_1, X_pred_2, X_pred_3, X_pred_4, X_pred_5), axis=0)\n",
    "train_pred_df = pd.DataFrame(train_pred)\n",
    "train_pred_df.columns = c\n",
    "train_pred_df.to_csv(\"train_lstm_X.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
